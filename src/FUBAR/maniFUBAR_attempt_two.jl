# ----------------------------------------------------------------------------
# â€” abstract "Prior" interface + dispatchers
# ----------------------------------------------------------------------------
abstract type Prior end

# â€” Forward declaration of the problem type
struct HierarchicalRMALAProblem{PÎ¼<:Prior, PÎ£<:Prior}
    grids
    prior_mu::PÎ¼
    prior_sigma::PÎ£
end

# â€” Normal prior on each Î¼áµ¢ ~ ð’©(0, Î£)
struct NormalPrior <: Prior end

function logprior(::NormalPrior, Î¼s::Vector{Vector{T}}, Î£::AbstractMatrix{T}) where T
    lp = 1/2*logdet(Î£)
    for Î¼ in Î¼s
        lp -= 1/2 * dot(Î¼, Î£ \ Î¼)
    end
    return lp
end

function grad_logprior(::NormalPrior, Î¼s::Vector{Vector{T}}, Î£::AbstractMatrix{T}) where T
    return [ -(Î£ \ Î¼) for Î¼ in Î¼s ]
end

function euclid_grad_logprior(::NormalPrior, Î¼s::Vector{Vector{T}}, Î£::AbstractMatrix{T}) where T
    N = length(Î¼s)
    S = zeros(eltype(Î£), size(Î£))
    for Î¼ in Î¼s
        S .+= Î¼ * Î¼'
    end
    t1 = -N/2 * inv(Î£)
    t2 = +1/2 * inv(Î£) * S * inv(Î£)
    return t1 .+ t2
end

# â€” Wishart prior Î£ ~ W(df, V0)
struct WishartPrior{T} <: Prior
    df::Int
    V0::Matrix{T}
end

function logprior(p::WishartPrior, Î£::AbstractMatrix{T}) where T
    Î½, V0 = p.df, p.V0; K = size(Î£,1)
    return ((Î½ - K - 1)/2)*logdet(Î£) - 1/2*tr(inv(V0)*Î£) 
end

function grad_logprior(p::WishartPrior, Î£::AbstractMatrix{T}) where T
    Î½, V0 = p.df, p.V0; K = size(Î£,1)
    return ((Î½ - K - 1)/2)*inv(Î£) .- 1/2*inv(V0)
end

# â€” Riemannianâ€Gaussian prior on SPD: p(Î£) âˆ exp(âˆ’d_RÂ²(Î£,M)/(2ÏƒÂ²))
struct RiemannianGaussianPrior{T} <: Prior
    M::Matrix{T}
    Ïƒ2::T
end

function logprior(p::RiemannianGaussianPrior, Î£::AbstractMatrix{T}) where T
    S  = sqrt(p.M); iS = inv(S)
    Î”  = log(iS * Î£ * iS)
    d2 = tr(Î” * Î”)
    return -d2/(2*p.Ïƒ2)
end

function grad_logprior(p::RiemannianGaussianPrior, Î£::AbstractMatrix{T}) where T
    S  = sqrt(p.M); iS = inv(S)
    Î”  = log(iS * Î£ * iS)
    return -1/p.Ïƒ2 * (iS * Î” * iS)
end

# ----------------------------------------------------------------------------
# â€” likelihood
# ----------------------------------------------------------------------------
function loglikelihood(p::HierarchicalRMALAProblem, Î¼s)
    ll = 0.0
    for (i, Î¼) in enumerate(Î¼s)
        L = p.grids[i].cond_lik_matrix
        log_soft = Î¼ .- logsumexp(Î¼)
        soft     = exp.(log_soft) / sum(exp.(log_soft))
        ll      += sum(log.(soft .* L))
    end
    return ll
end

# ----------------------------------------------------------------------------
# â€” full problem type
# ----------------------------------------------------------------------------
# Delete this duplicate struct definition
# struct HierarchicalRMALAProblem{PÎ¼<:Prior, PÎ£<:Prior}
#     grids::Vector{FUBARgrid}
#     prior_mu::PÎ¼
#     prior_sigma::PÎ£
# end

# ----------------------------------------------------------------------------
# â€” posterior + grads
# ----------------------------------------------------------------------------
function log_posterior(p::HierarchicalRMALAProblem, Î¼s, Î£)
    return loglikelihood(p, Î¼s) +
           logprior(p.prior_mu, Î¼s, Î£) +
           logprior(p.prior_sigma, Î£)
end

function grad_log_post_mu(p::HierarchicalRMALAProblem, Î¼s, Î£)
    # Compute gradient of ll = sum_j log(sum_i L[i,j] * softmax(Î¼)[i])
    gs = [ zeros(length(Î¼)) for Î¼ in Î¼s ]
    for (i, Î¼) in enumerate(Î¼s)
        L = p.grids[i].cond_lik_matrix   # KÃ—N
        # softmax probabilities
        v = exp.(Î¼ .- logsumexp(Î¼))
        v ./= sum(v)
        # mixture likelihood per site: M_j = âˆ‘_i L[i,j] * v[i]
        M = v' * L                     # 1Ã—N row vector
        # gradient wrt v: âˆ‚/âˆ‚v sum_j log(M_j) = L * (1 ./ M)'
        w = vec(1.0 ./ M)             # N-vector
        grad_v = L * w                # K-vector
        # propagate through softmax: âˆ‡_Î¼ = J_softmax^T * grad_v
        Î± = dot(v, grad_v)
        gs[i] = v .* (grad_v .- Î±)
    end
    # add Î¼-prior gradient
    gÎ¼_prior = grad_logprior(p.prior_mu, Î¼s, Î£)
    return [g + gp for (g, gp) in zip(gs, gÎ¼_prior)]
end

function euclid_grad_Sigma(p::HierarchicalRMALAProblem, Î¼s, Î£)
    return euclid_grad_logprior(p.prior_mu, Î¼s, Î£) .+
           grad_logprior(p.prior_sigma, Î£)
end

riemannian_grad_Sigma(p, Î¼s, Î£) = Î£ * euclid_grad_Sigma(p, Î¼s, Î£) * Î£

# ----------------------------------------------------------------------------
# â€” manifold ops
# ----------------------------------------------------------------------------
exp_map_Sigma(Î£, V) = begin
    S  = sqrt(Î£); iS = inv(S)
    return S * exp(iS * V * iS) * S
end

log_map_Sigma(Î£, T) = begin
    S  = sqrt(Î£); iS = inv(S)
    return S * log(iS * T * iS) * S
end

metric_inner_Sigma(Î£, U, V) = tr(inv(Î£)*U*inv(Î£)*V)

function rand_tangent_Sigma(Î£, Ï„)
    K = size(Î£,1)
    W = randn(K,K); symW = (W + W')/âˆš2; S = sqrt(Î£)
    return âˆšÏ„ * (S * symW * S)
end

log_q_mu(Î¼â‚€, Î¼â‚, gâ‚€, Ï„) = begin Î” = Î¼â‚ .- Î¼â‚€ .- (Ï„/2)*gâ‚€; -dot(Î”,Î”)/(2Ï„) end
log_q_Sigma(Î£â‚€, Î£â‚, gâ‚€, Ï„) = begin
    V = log_map_Sigma(Î£â‚€, Î£â‚); Î” = V .- (Ï„/2)*gâ‚€
    -metric_inner_Sigma(Î£â‚€, Î”, Î”)/(2Ï„)
end

# ----------------------------------------------------------------------------
# â€” RMALA iteration
# ----------------------------------------------------------------------------
function rmala_step(p::HierarchicalRMALAProblem, Î¼s, Î£, Ï„)
    gÎ¼  = grad_log_post_mu(p, Î¼s, Î£)
    gÎ£  = riemannian_grad_Sigma(p, Î¼s, Î£)

    Î¼sâ€² = [ Î¼ .+ (Ï„/2)*g .+ âˆšÏ„*randn(length(Î¼)) for (Î¼,g) in zip(Î¼s, gÎ¼) ]

    Î¶Î£    = rand_tangent_Sigma(Î£, Ï„)
    Î£â€²    = exp_map_Sigma(Î£, (Ï„/2)*gÎ£ .+ Î¶Î£)

    lp0   = log_posterior(p, Î¼s, Î£)
    lp1   = log_posterior(p, Î¼sâ€², Î£â€²)

    lqf   = sum(log_q_mu(Î¼, Î¼p, g, Ï„) for (Î¼,Î¼p,g) in zip(Î¼s, Î¼sâ€², gÎ¼)) +
            log_q_Sigma(Î£, Î£â€², gÎ£, Ï„)

    gÎ¼â€²   = grad_log_post_mu(p, Î¼sâ€², Î£â€²)
    gÎ£â€²   = riemannian_grad_Sigma(p, Î¼sâ€², Î£â€²)

    lqr   = sum(log_q_mu(Î¼p, Î¼, gp, Ï„) for (Î¼,Î¼p,gp) in zip(Î¼s, Î¼sâ€², gÎ¼â€²)) +
            log_q_Sigma(Î£â€², Î£, gÎ£â€², Ï„)

    logÎ± = lp1 - lp0 + (lqr - lqf)
    if log(rand()) < logÎ±
        return Î¼sâ€², Î£â€², true
    else
        return Î¼s, Î£, false
    end
end

# ----------------------------------------------------------------------------
# â€” full sampler (separate returns)
# ----------------------------------------------------------------------------
function rmala_sampler(p::HierarchicalRMALAProblem,
                       Î¼s0::Vector{Vector{Float64}},
                       Î£0::Matrix{Float64};
                       Ï„::Float64=1e-4, num_samples::Int=1000, burnin::Int=0)
    Î¼s, Î£ = deepcopy(Î¼s0), deepcopy(Î£0)
    mus_samples   = Vector{Vector{Vector{Float64}}}()
    sigma_samples = Vector{Matrix{Float64}}()
    logpost_samples = Vector{Float64}()
    acc = 0
    total_iter = 0
    
    # Initial log posterior
    current_logpost = log_posterior(p, Î¼s, Î£)
    
    while length(mus_samples) < num_samples
        total_iter += 1
        Î¼s_new, Î£_new, accepted = rmala_step(p, Î¼s, Î£, Ï„)
        
        if accepted
            acc += 1
            Î¼s, Î£ = Î¼s_new, Î£_new
            current_logpost = log_posterior(p, Î¼s, Î£)
            
            # Only store samples after burnin
            if total_iter > burnin
                push!(mus_samples, deepcopy(Î¼s))
                push!(sigma_samples, deepcopy(Î£))
                push!(logpost_samples, current_logpost)
            end
        end
        
        # Print debug info every 100 iterations
        if total_iter % 100 == 0
            @info "Iter $(total_iter): Acceptance rate = $(acc/total_iter), Collected $(length(mus_samples))/$(num_samples) samples"
        end
    end
    
    @info "Final acceptance rate = $(acc/total_iter), Total iterations = $(total_iter)"
    return mus_samples, sigma_samples, logpost_samples
end
